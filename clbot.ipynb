{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computational Linguistics Teaching Assistant\n",
    "A chatbot that answers questions about NLP using Stanford's CS224N lecture content.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytube openai-whisper langchain openai yt-dlp pinecone-client python-dotenv\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-pinecone\n",
    "!pip install -U langchain-openai langchain-core\n",
    "!pip install torch==2.1.1+cu121 torchvision==0.16.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install huggingface_hub==0.11.1\n",
    "!pip install -U pinecone-client\n",
    "!pip install pydantic\n",
    "!pip install python-dotenv\n",
    "!pip install -U sentence-transformers\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install langsmith\n",
    "!pip install --upgrade gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Type, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent, initialize_agent\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION ENVIRONMENT VARIABLES\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Define environment variables\n",
    "env_content = \"\"\"\n",
    "OPENAI_API_KEY=...\n",
    "PINECONE_API_KEY=...\n",
    "PINECONE_INDEX_NAME=...\n",
    "PINECONE_INDEX_URL=...\n",
    "LANGCHAIN_TRACING_V2=...\n",
    "LANGCHAIN_ENDPOINT=h...\n",
    "LANGCHAIN_API_KEY=l...\n",
    "LANGCHAIN_PROJECT=...\n",
    "\"\"\"\n",
    "\n",
    "# Write environment variables\n",
    "env_file_path = \"/notebooks/.env\"\n",
    "with open(env_file_path, 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# Load and verify environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME')\n",
    "PINECONE_INDEX_URL = os.getenv('PINECONE_INDEX_URL')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Verify API keys are loaded\n",
    "print(\"OpenAI API Key Loaded:\", bool(OPENAI_API_KEY))\n",
    "print(\"Pinecone API Key Loaded:\", bool(PINECONE_API_KEY))\n",
    "print(\"LangChain API Key Loaded:\", bool(LANGCHAIN_API_KEY))\n",
    "\n",
    "# Initialize base clients\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUTUBE DOWNDLOAD AND TRANSCRIPTION SETUP\n",
    "# Set up storage paths\n",
    "NOTEBOOKS_PATH = \"/notebooks\"\n",
    "STORAGE_PATH = os.path.join(NOTEBOOKS_PATH, \"youtube_transcripts\")\n",
    "DOWNLOADS_DIR = os.path.join(STORAGE_PATH, \"downloads\")\n",
    "TRANSCRIPTS_DIR = os.path.join(STORAGE_PATH, \"transcripts\")\n",
    "MODEL_CACHE_DIR = os.path.join(STORAGE_PATH, \"model_cache\")\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [DOWNLOADS_DIR, TRANSCRIPTS_DIR, MODEL_CACHE_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Created directory: {directory}\")\n",
    "\n",
    "def read_urls_from_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Read URLs from a text file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        urls = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    return urls\n",
    "\n",
    "def download_audio(url: str, lecture_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Download audio from YouTube URL with sequential lecture numbering.\n",
    "    \"\"\"\n",
    "    output_template = os.path.join(DOWNLOADS_DIR, f'Lecture{lecture_number}')\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192'\n",
    "        }],\n",
    "        'outtmpl': output_template,\n",
    "        'quiet': False,\n",
    "        'no_warnings': False\n",
    "    }\n",
    "    \n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            ydl.download([url])\n",
    "            audio_path = f\"{output_template}.mp3\"\n",
    "            return audio_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def transcribe_audio(audio_path: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Transcribe audio and save the transcript to a file.\n",
    "    Returns tuple of (transcript text, transcript file path)\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting transcription process for {audio_path}\")\n",
    "    print(\"Loading Whisper model...\")\n",
    "    os.environ['WHISPER_CACHE_DIR'] = MODEL_CACHE_DIR\n",
    "    \n",
    "    try:\n",
    "        model = whisper.load_model(\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Model loaded successfully. Beginning transcription...\")\n",
    "        \n",
    "        result = model.transcribe(audio_path)\n",
    "        print(\"Transcription completed successfully\")\n",
    "        \n",
    "        base_name = os.path.basename(audio_path).replace('.mp3', '')\n",
    "        transcript_path = os.path.join(TRANSCRIPTS_DIR, f\"{base_name}_transcript.txt\")\n",
    "        \n",
    "        print(f\"Saving transcript to {transcript_path}\")\n",
    "        with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result['text'])\n",
    "        print(\"Transcript saved successfully\")\n",
    "        \n",
    "        return result['text'], transcript_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class BatchDownloadAndTranscribeTool(BaseTool):\n",
    "    name: str = \"Batch Download and Transcribe\"\n",
    "    description: str = \"Downloads audio from multiple YouTube URLs and transcribes them to text. Input can be either a single URL or path to a text file containing URLs.\"\n",
    "    \n",
    "    def _run(self, input_path_or_url: str) -> str:\n",
    "        \"\"\"Run the tool.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting batch process for input: {input_path_or_url}\")\n",
    "            \n",
    "            if input_path_or_url.endswith('.txt'):\n",
    "                urls = read_urls_from_file(input_path_or_url)\n",
    "                print(f\"Found {len(urls)} URLs in file\")\n",
    "            else:\n",
    "                urls = [input_path_or_url]\n",
    "            \n",
    "            results = []\n",
    "            for i, url in enumerate(urls, 1):\n",
    "                print(f\"\\nProcessing URL {i}/{len(urls)}: {url}\")\n",
    "                try:\n",
    "                    print(f\"Step 1: Downloading audio for Lecture {i}...\")\n",
    "                    audio_path = download_audio(url, i)\n",
    "                    print(f\"‚úì Audio downloaded to: {audio_path}\")\n",
    "                    \n",
    "                    print(f\"Step 2: Transcribing Lecture {i}...\")\n",
    "                    transcript, transcript_path = transcribe_audio(audio_path)\n",
    "                    print(f\"‚úì Transcript saved to: {transcript_path}\")\n",
    "                    \n",
    "                    audio_size = os.path.getsize(audio_path) / (1024 * 1024)\n",
    "                    transcript_size = os.path.getsize(transcript_path) / 1024\n",
    "                    \n",
    "                    results.append({\n",
    "                        'lecture_number': i,\n",
    "                        'url': url,\n",
    "                        'audio_path': audio_path,\n",
    "                        'audio_size': audio_size,\n",
    "                        'transcript_path': transcript_path,\n",
    "                        'transcript_size': transcript_size,\n",
    "                        'success': True\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Lecture {i}: {str(e)}\")\n",
    "                    results.append({\n",
    "                        'lecture_number': i,\n",
    "                        'url': url,\n",
    "                        'error': str(e),\n",
    "                        'success': False\n",
    "                    })\n",
    "            \n",
    "            report = \"\\nBatch Processing Summary:\\n\"\n",
    "            report += \"=\" * 50 + \"\\n\"\n",
    "            for result in results:\n",
    "                report += f\"\\nLecture {result['lecture_number']}:\\n\"\n",
    "                report += f\"URL: {result['url']}\\n\"\n",
    "                if result['success']:\n",
    "                    report += f\"Audio ({result['audio_size']:.2f}MB): {result['audio_path']}\\n\"\n",
    "                    report += f\"Transcript ({result['transcript_size']:.2f}KB): {result['transcript_path']}\\n\"\n",
    "                else:\n",
    "                    report += f\"Failed: {result['error']}\\n\"\n",
    "            \n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"An error occurred during batch processing: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    def _arun(self, url: str) -> str:\n",
    "        \"\"\"Run the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "# Initialize the tool and agent\n",
    "batch_download_tool = BatchDownloadAndTranscribeTool()\n",
    "\n",
    "# Helper function to format tools\n",
    "def format_tools(tools):\n",
    "    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "\n",
    "# Initialize memory for conversational context\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Define the prompt template with tool instructions\n",
    "template = \"\"\"You are a helpful assistant that processes YouTube videos for transcription.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    partial_variables={\"tools\": lambda x: format_tools(tools), \"tool_names\": lambda x: \", \".join(t.name for t in tools)}\n",
    ")\n",
    "\n",
    "# Initialize the agent with the tool\n",
    "tools = [batch_download_tool]\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Reset directories if needed\n",
    "for dir_path in [DOWNLOADS_DIR, TRANSCRIPTS_DIR]:\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"YouTube download and transcription setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSCRIPT PROCESSING\n",
    "\n",
    "# Define lecture information and metadata\n",
    "LECTURE_INFO = {\n",
    "    1: {\n",
    "        'title': 'Natural Language Processing with Deep Learning',\n",
    "        'main_topics': ['NLP basics', 'Word Vectors'],\n",
    "        'key_concepts': ['Natural Language Processing', 'Word Vectors', 'Singular Value Decomposition', \n",
    "                        'Skip-gram', 'Continuous Bag of Words', 'Negative Sampling', \n",
    "                        'Hierarchical Softmax', 'Word2Vec'],\n",
    "        'builds_on': []  # First lecture, no prerequisites\n",
    "    },\n",
    "    2: {\n",
    "        'title': 'Word Vector Representations: word2vec',\n",
    "        'main_topics': ['Word Vector Implementation', 'Word2Vec Details'],\n",
    "        'key_concepts': ['Word Vectors', 'Skip-gram', 'Continuous Bag of Words', \n",
    "                        'Negative Sampling', 'Hierarchical Softmax', 'Word2Vec'],\n",
    "        'builds_on': ['Word Vectors', 'Natural Language Processing']\n",
    "    },\n",
    "    3: {\n",
    "        'title': 'GloVe: Global Vectors for Word Representation',\n",
    "        'main_topics': ['GloVe', 'Word Vector Evaluation'],\n",
    "        'key_concepts': ['GloVe', 'Intrinsic evaluation', 'Extrinsic evaluation', \n",
    "                        'Word analogies', 'Context windows', 'Window classification'],\n",
    "        'builds_on': ['Word Vectors', 'Word2Vec']\n",
    "    },\n",
    "    4: {\n",
    "        'title': 'Word Window Classification and Neural Networks',\n",
    "        'main_topics': ['Neural Networks', 'Classification'],\n",
    "        'key_concepts': ['Neural networks', 'Forward computation', 'Backward propagation',\n",
    "                        'Neuron Units', 'Max-margin Loss', 'Gradient checks', \n",
    "                        'Xavier initialization', 'Learning rates', 'Adagrad'],\n",
    "        'builds_on': ['Window classification']\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_transcripts(transcripts_dir: str) -> Dict[int, str]:\n",
    "    \"\"\"Load all transcripts from the directory.\"\"\"\n",
    "    transcripts = {}\n",
    "    transcript_files = sorted(Path(transcripts_dir).glob(\"*_transcript.txt\"))\n",
    "    \n",
    "    for file_path in transcript_files:\n",
    "        if not str(file_path).endswith('.ipynb_checkpoints'):\n",
    "            lecture_num = int(file_path.name.split('_')[0].replace('Lecture', ''))\n",
    "            with open(file_path, 'r') as f:\n",
    "                transcripts[lecture_num] = f.read()\n",
    "    \n",
    "    print(f\"Loaded {len(transcripts)} transcripts\")\n",
    "    return transcripts\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = 1500, overlap: int = 200) -> List[Dict]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Determine end position of current chunk\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # If we're not at the end of the text, find the next period for clean breaks\n",
    "        if end < len(text):\n",
    "            next_period = text[end:min(end + 100, len(text))].find('.')\n",
    "            if next_period != -1:\n",
    "                end = end + next_period + 1\n",
    "        \n",
    "        chunk = text[start:end].strip()\n",
    "        chunks.append({\n",
    "            'content': chunk,\n",
    "            'char_start': start,\n",
    "            'char_end': end\n",
    "        })\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Main processing function\n",
    "def process_all_transcripts():\n",
    "    \"\"\"Process all transcripts and create structured chunks with metadata.\"\"\"\n",
    "    print(\"\\nStarting transcript processing...\")\n",
    "    \n",
    "    # Load all transcripts\n",
    "    transcripts = load_transcripts(TRANSCRIPTS_DIR)\n",
    "    \n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Process each lecture\n",
    "    for lecture_num, text in transcripts.items():\n",
    "        print(f\"\\nProcessing Lecture {lecture_num}...\")\n",
    "        \n",
    "        # Get lecture metadata\n",
    "        lecture_info = LECTURE_INFO.get(lecture_num, {\n",
    "            'title': f'Lecture {lecture_num}',\n",
    "            'key_concepts': [],\n",
    "            'main_topics': []\n",
    "        })\n",
    "        \n",
    "        # Create chunks for this lecture\n",
    "        chunks = create_chunks(text)\n",
    "        \n",
    "        # Add metadata to each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                'lecture_number': lecture_num,\n",
    "                'lecture_title': lecture_info['title'],\n",
    "                'key_concepts': lecture_info['key_concepts'],\n",
    "                'main_topics': lecture_info['main_topics'],\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks),\n",
    "                'content': chunk['content'],\n",
    "                'char_start': chunk['char_start'],\n",
    "                'char_end': chunk['char_end']\n",
    "            }\n",
    "            processed_chunks.append(chunk_data)\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks for Lecture {lecture_num}\")\n",
    "    \n",
    "    # Save processed chunks\n",
    "    output_dir = os.path.join(STORAGE_PATH, \"processed\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'lecture_chunks.json')\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(processed_chunks, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total chunks created: {len(processed_chunks)}\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "# Execute the processing\n",
    "if __name__ == \"__main__\":\n",
    "    processed_chunks = process_all_transcripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING GENERATION\n",
    "\n",
    "# Initialize OpenAI client for embeddings\n",
    "print(\"Initializing embedding generation...\")\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"Generate embedding for text using OpenAI's API\"\"\"\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_chunks_with_embeddings():\n",
    "    \"\"\"Process all chunks and add embeddings\"\"\"\n",
    "    # Load chunks\n",
    "    input_file = os.path.join(STORAGE_PATH, \"processed\", \"lecture_chunks.json\")\n",
    "    print(f\"Loading chunks from {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        print(f\"Loaded {len(chunks)} chunks\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Processed chunks file not found. Please run transcript processing first.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON file\")\n",
    "        return\n",
    "\n",
    "    # Process chunks and add embeddings\n",
    "    chunks_with_embeddings = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(\"\\nGenerating embeddings for all chunks...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"Processing chunk {i}/{total_chunks}\", end='\\r')\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = get_embedding(chunk['content'])\n",
    "        \n",
    "        if embedding:\n",
    "            # Add embedding to chunk data\n",
    "            chunk_with_embedding = chunk.copy()\n",
    "            chunk_with_embedding['embedding'] = embedding\n",
    "            chunks_with_embeddings.append(chunk_with_embedding)\n",
    "        else:\n",
    "            print(f\"\\nWarning: Failed to generate embedding for chunk {i}\")\n",
    "\n",
    "    # Save embeddings\n",
    "    output_file = os.path.join(STORAGE_PATH, \"processed\", \"lecture_chunks_with_embeddings.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(chunks_with_embeddings, f)\n",
    "\n",
    "    print(f\"\\nEmbedding generation complete!\")\n",
    "    print(f\"Processed {len(chunks_with_embeddings)} chunks with embeddings\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    # Print sample embedding dimension\n",
    "    if chunks_with_embeddings:\n",
    "        embedding_dim = len(chunks_with_embeddings[0]['embedding'])\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    return chunks_with_embeddings\n",
    "\n",
    "def verify_embeddings(chunks_with_embeddings):\n",
    "    \"\"\"Verify the quality and consistency of generated embeddings\"\"\"\n",
    "    if not chunks_with_embeddings:\n",
    "        print(\"No embeddings to verify\")\n",
    "        return False\n",
    "\n",
    "    expected_dim = 1536  # Expected dimension for OpenAI ada-002 embeddings\n",
    "    all_valid = True\n",
    "\n",
    "    print(\"\\nVerifying embeddings...\")\n",
    "    for i, chunk in enumerate(chunks_with_embeddings):\n",
    "        # Check if embedding exists\n",
    "        if 'embedding' not in chunk:\n",
    "            print(f\"Chunk {i} missing embedding\")\n",
    "            all_valid = False\n",
    "            continue\n",
    "\n",
    "        # Check embedding dimension\n",
    "        embedding_dim = len(chunk['embedding'])\n",
    "        if embedding_dim != expected_dim:\n",
    "            print(f\"Chunk {i} has incorrect dimension: {embedding_dim} (expected {expected_dim})\")\n",
    "            all_valid = False\n",
    "\n",
    "        # Check for null values\n",
    "        if None in chunk['embedding']:\n",
    "            print(f\"Chunk {i} contains null values in embedding\")\n",
    "            all_valid = False\n",
    "\n",
    "    if all_valid:\n",
    "        print(\"‚úì All embeddings verified successfully!\")\n",
    "    else:\n",
    "        print(\"√ó Some embeddings failed verification\")\n",
    "\n",
    "    return all_valid\n",
    "\n",
    "# Execute embedding generation and verification\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting embedding generation process...\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    chunks_with_embeddings = process_chunks_with_embeddings()\n",
    "    \n",
    "    # Verify embeddings\n",
    "    if chunks_with_embeddings:\n",
    "        verify_embeddings(chunks_with_embeddings)\n",
    "    \n",
    "    print(\"\\nEmbedding process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINECONE SET UP AND VECTOR STORAGE\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from tqdm import tqdm\n",
    "\n",
    "def initialize_pinecone(index_name):\n",
    "    \"\"\"Initialize Pinecone client and connect to index\"\"\"\n",
    "    print(\"\\nInitializing Pinecone...\")\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if index exists, if not create it\n",
    "        try:\n",
    "            index = pc.Index(index_name)\n",
    "            print(f\"Connected to existing Pinecone index: {index_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Index not found, creating new index: {index_name}\")\n",
    "            # Create index with serverless spec\n",
    "            spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=1536,  # OpenAI ada-002 embedding dimension\n",
    "                metric=\"cosine\",\n",
    "                spec=spec\n",
    "            )\n",
    "            index = pc.Index(index_name)\n",
    "            print(f\"Created new Pinecone index: {index_name}\")\n",
    "        \n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_vectors_for_upsert(chunks_with_embeddings):\n",
    "    \"\"\"Prepare vectors in the format required by Pinecone\"\"\"\n",
    "    vectors = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_with_embeddings):\n",
    "        # Create unique ID for each vector\n",
    "        vector_id = f\"chunk_{chunk['lecture_number']}_{chunk['chunk_index']}\"\n",
    "        \n",
    "        # Prepare metadata - Store main content in 'content' field\n",
    "        metadata = {\n",
    "            'lecture_number': chunk['lecture_number'],\n",
    "            'lecture_title': chunk['lecture_title'],\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'key_concepts': chunk['key_concepts'],\n",
    "            'content': chunk['content']  # Main content stored here\n",
    "        }\n",
    "        \n",
    "        # Create vector object\n",
    "        vector = {\n",
    "            'id': vector_id,\n",
    "            'values': chunk['embedding'],\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def upsert_to_pinecone(index, vectors, batch_size=100):\n",
    "    \"\"\"Upsert vectors to Pinecone in batches\"\"\"\n",
    "    print(\"\\nUpserting vectors to Pinecone...\")\n",
    "    total_vectors = len(vectors)\n",
    "    \n",
    "    for i in range(0, total_vectors, batch_size):\n",
    "        batch = vectors[i:min(i + batch_size, total_vectors)]\n",
    "        try:\n",
    "            index.upsert(vectors=batch)\n",
    "            print(f\"Upserted batch {i//batch_size + 1}/{(total_vectors + batch_size - 1)//batch_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting batch starting at index {i}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def verify_pinecone_index(index):\n",
    "    \"\"\"Verify the index statistics and content\"\"\"\n",
    "    try:\n",
    "        # Get index stats\n",
    "        stats = index.describe_index_stats()\n",
    "        \n",
    "        print(\"\\nPinecone Index Statistics:\")\n",
    "        print(f\"Total vectors: {stats.total_vector_count}\")\n",
    "        print(f\"Dimension: {stats.dimension}\")\n",
    "        \n",
    "        # Perform a test query with embedding\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        test_query = embedding_model.embed_query(\"test\")\n",
    "        \n",
    "        results = index.query(\n",
    "            vector=test_query,\n",
    "            top_k=1,\n",
    "            include_values=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        if results.matches:\n",
    "            print(\"\\nTest query successful!\")\n",
    "            print(\"Sample document metadata:\")\n",
    "            metadata = results.matches[0].metadata\n",
    "            print(\"\\nMetadata fields found:\")\n",
    "            for key in metadata.keys():\n",
    "                if key == 'content':\n",
    "                    print(f\"{key}: <content length: {len(str(metadata[key]))} chars>\")\n",
    "                else:\n",
    "                    print(f\"{key}: {metadata[key]}\")\n",
    "        else:\n",
    "            print(\"Test query returned no results\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying index: {e}\")\n",
    "        return False\n",
    "\n",
    "def initialize_vector_store(index):\n",
    "    \"\"\"Initialize LangChain's vector store wrapper for Pinecone\"\"\"\n",
    "    try:\n",
    "        vector_store = PineconeVectorStore(\n",
    "            index=index,\n",
    "            embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "            text_key=\"content\"  # Using 'content' as the text key\n",
    "        )\n",
    "        print(\"\\nLangChain vector store initialized successfully\")\n",
    "        \n",
    "        # Verify vector store works\n",
    "        test_results = vector_store.similarity_search(\"test\", k=1)\n",
    "        if test_results:\n",
    "            print(\"Vector store retrieval test successful\")\n",
    "            print(f\"Retrieved document length: {len(test_results[0].page_content)}\")\n",
    "        \n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_pinecone_setup(mode='connect'):\n",
    "    \"\"\"Main function to set up Pinecone and upload vectors\"\"\"\n",
    "    # Initialize Pinecone\n",
    "    index = initialize_pinecone(PINECONE_INDEX_NAME)\n",
    "    if not index:\n",
    "        return False\n",
    "        \n",
    "    if mode == 'new':\n",
    "        # Load chunks with embeddings\n",
    "        input_file = os.path.join(STORAGE_PATH, \"processed\", \"lecture_chunks_with_embeddings.json\")\n",
    "        try:\n",
    "            with open(input_file, 'r') as f:\n",
    "                chunks_with_embeddings = json.load(f)\n",
    "            print(f\"\\nLoaded {len(chunks_with_embeddings)} chunks with embeddings\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings file: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Prepare and upsert vectors\n",
    "        vectors = prepare_vectors_for_upsert(chunks_with_embeddings)\n",
    "        print(f\"Prepared {len(vectors)} vectors for upload\")\n",
    "        \n",
    "        if not upsert_to_pinecone(index, vectors):\n",
    "            return False\n",
    "    \n",
    "    # Verify the index\n",
    "    if not verify_pinecone_index(index):\n",
    "        return False\n",
    "    \n",
    "    # Initialize vector store\n",
    "    vector_store = initialize_vector_store(index)\n",
    "    if not vector_store:\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nPinecone setup completed successfully!\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose setup mode: 'new' for fresh setup, 'connect' for existing index\n",
    "    setup_mode = 'connect'  # or 'new'\n",
    "    \n",
    "    vector_store = main_pinecone_setup(mode=setup_mode)\n",
    "    \n",
    "    if vector_store:\n",
    "        print(\"\\nSetup complete! The vector store is ready for querying.\")\n",
    "        \n",
    "        # Verification step\n",
    "        print(\"\\nPerforming final verification...\")\n",
    "        try:\n",
    "            test_docs = vector_store.similarity_search(\"test query\", k=1)\n",
    "            if test_docs:\n",
    "                print(\"‚úÖ Vector store is working correctly\")\n",
    "                print(f\"Sample document metadata keys: {list(test_docs[0].metadata.keys())}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Vector store returned no results\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in final verification: {e}\")\n",
    "    else:\n",
    "        print(\"\\nSetup failed. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index(os.environ[\"PINECONE_INDEX_NAME\"])\n",
    "\n",
    "# Create a simple vector of the correct dimension (1536 for Ada)\n",
    "test_vector = [0.0] * 1536\n",
    "\n",
    "# Query the index\n",
    "results = index.query(\n",
    "    vector=test_vector,\n",
    "    top_k=1,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Query Results:\")\n",
    "print(\"=============\")\n",
    "if results.matches:\n",
    "    print(\"\\nFirst document metadata:\")\n",
    "    print(results.matches[0].metadata)\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "\n",
    "def inspect_pinecone():\n",
    "    # Initialize Pinecone\n",
    "    print(\"\\n1. Connecting to Pinecone...\")\n",
    "    pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "    \n",
    "    # Get index\n",
    "    index_name = os.environ[\"PINECONE_INDEX_NAME\"]\n",
    "    print(f\"\\n2. Accessing index: {index_name}\")\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    # Create a simple test vector\n",
    "    print(\"\\n3. Creating test query...\")\n",
    "    test_vector = np.zeros(1536)  # OpenAI embeddings are 1536 dimensions\n",
    "    \n",
    "    # Perform a simple query\n",
    "    print(\"\\n4. Performing test query...\")\n",
    "    response = index.query(\n",
    "        vector=test_vector.tolist(),\n",
    "        top_k=1,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    if response and response.matches:\n",
    "        print(\"\\n5. Sample document metadata:\")\n",
    "        match = response.matches[0]\n",
    "        print(\"\\nMetadata fields:\")\n",
    "        for key, value in match.metadata.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                print(f\"{key}: <text length: {len(value)} chars>\")\n",
    "                print(\"First 100 characters:\", value[:100])\n",
    "            else:\n",
    "                print(f\"{key}:\", value)\n",
    "    else:\n",
    "        print(\"\\nNo matches found in the index.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        inspect_pinecone()\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://49388f5b7d6d092856.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://49388f5b7d6d092856.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question: what does NLP mean?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 relevant lectures\n",
      "Found 3 key concepts\n",
      "\n",
      "Processing question: What is cross entropy?\n",
      "Found 3 relevant lectures\n",
      "Found 9 key concepts\n",
      "\n",
      "Processing question: How is it used in neural networks?\n",
      "Found 3 relevant lectures\n",
      "Found 9 key concepts\n",
      "\n",
      "Processing question: What other loss functions are mentioned in the neural networks lecture?\n",
      "\n",
      "Processing question: What is BERT?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question: What methods for word representation ARE covered in the lectures?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant lectures\n",
      "Found 5 key concepts\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import List\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize components \n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index(os.environ[\"PINECONE_INDEX_NAME\"])\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"content\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=3,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Modified prompt template to enforce strict retrieval\n",
    "custom_template = \"\"\"\n",
    "You are a helpful teaching assistant for a course on Computational Linguistics.\n",
    "You must ONLY answer using information from the provided context. \n",
    "If the context doesn't contain enough information to answer the question properly, respond with \"I can't find specific information about this in the course materials.\"\n",
    "Do not use any external knowledge or make assumptions beyond what's in the context.\n",
    "\n",
    "Current conversation:\n",
    "{chat_history}\n",
    "\n",
    "Context from course materials:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Check if the context contains relevant information to answer the question\n",
    "2. If sufficient information exists:\n",
    "   - Connect your answer to specific lectures\n",
    "   - Use only information from the provided context\n",
    "   - Include key concepts mentioned in the context\n",
    "3. If insufficient information exists:\n",
    "   - Respond with \"I can't find specific information about this in the course materials.\"\n",
    "4. Never make up or infer information not present in the context\n",
    "\n",
    "Student Question: {question}\n",
    "\n",
    "Teaching Assistant Answer:\"\"\"\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    template=custom_template, \n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': CUSTOM_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def chat(message: str) -> str:\n",
    "    \"\"\"Process a single message and return the response\"\"\"\n",
    "    print(f\"\\nProcessing question: {message}\")\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    try:\n",
    "        # Get response\n",
    "        response = qa_chain({\"question\": message})\n",
    "        \n",
    "        # Check if we got any source documents\n",
    "        if not response.get('source_documents'):\n",
    "            return \"I can't find specific information about this in the course materials.\"\n",
    "        \n",
    "        # Process the response\n",
    "        answer = response[\"answer\"].strip()\n",
    "        \n",
    "        # If the answer indicates no information was found, return early\n",
    "        if \"can't find specific information\" in answer.lower():\n",
    "            return answer\n",
    "        \n",
    "        # Add sources and concepts if we have a valid answer\n",
    "        sources = set()\n",
    "        concepts = set()\n",
    "        \n",
    "        for doc in response['source_documents']:\n",
    "            if hasattr(doc, 'metadata'):\n",
    "                lecture_num = doc.metadata.get('lecture_number')\n",
    "                lecture_title = doc.metadata.get('lecture_title')\n",
    "                if lecture_num and lecture_title:\n",
    "                    sources.add(f\"Lecture {lecture_num}: {lecture_title}\")\n",
    "                \n",
    "                if 'key_concepts' in doc.metadata:\n",
    "                    concepts.update(doc.metadata['key_concepts'])\n",
    "        \n",
    "        # Add metadata to response\n",
    "        formatted_response = answer\n",
    "        if sources:\n",
    "            formatted_response += \"\\n\\nüìö Sources:\\n\" + \"\\n\".join(f\"‚Ä¢ {s}\" for s in sorted(sources))\n",
    "        if concepts:\n",
    "            formatted_response += \"\\n\\nüîë Key Concepts:\\n‚Ä¢ \" + \", \".join(sorted(concepts))\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"Found {len(sources)} relevant lectures\")\n",
    "        print(f\"Found {len(concepts)} key concepts\")\n",
    "        \n",
    "        return formatted_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return f\"‚ùå Error: {str(e)}\\nPlease try asking your question again.\"\n",
    "\n",
    "# Create the Gradio interface - Fixed indentation\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Friendly header\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üéì Computational Linguistics Teaching Assistant\n",
    "    \n",
    "    Welcome! I'm your CS teaching assistant, trained on Stanford's lectures. \n",
    "    I can help you understand:\n",
    "    - üìö Word Vectors and Embeddings\n",
    "    - üß† Neural Networks in NLP\n",
    "    - ü§ñ GloVe and Word Representations\n",
    "    \n",
    "    Ask me anything about these topics!\n",
    "    \"\"\")\n",
    "\n",
    "    # Main interface\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            question = gr.Textbox(\n",
    "                placeholder=\"üåü No question is too simple or too complex - I'm here to help!\",\n",
    "                label=\"Your Question\",\n",
    "                lines=3\n",
    "            )\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            answer = gr.Textbox(\n",
    "                label=\"Assistant Response\",\n",
    "                lines=15,\n",
    "                interactive=False\n",
    "            )\n",
    "\n",
    "    # Handle events\n",
    "    def clear_fields():\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    submit.click(fn=chat, inputs=question, outputs=answer)\n",
    "    question.submit(fn=chat, inputs=question, outputs=answer)\n",
    "    clear.click(fn=clear_fields, inputs=[], outputs=[question, answer])\n",
    "\n",
    "    # Example Questions\n",
    "    gr.Markdown(\"### üí° Not sure where to start?\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"ü§î What is cross entropy?\",\n",
    "            \"‚ú® How do neural networks work in NLP?\",\n",
    "            \"üîç What is GloVe and what problem does it solve?\",\n",
    "        ],\n",
    "        inputs=question,\n",
    "        label=\"Try these questions\"\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
