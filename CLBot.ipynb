{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computational Linguistics Teaching Assistant\n",
    "A chatbot that answers questions about NLP using Stanford's CS224N lecture content.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
      "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.54.3)\n",
      "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2024.11.4)\n",
      "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.66.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper) (8.10.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.8.0)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.1.141)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client) (2020.6.20)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.0.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain-community) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.15)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.1.141)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.3)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain-community) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain-community) (2020.6.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (1.1.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-pinecone in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: aiohttp<3.10,>=3.9.5 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.3.15)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (1.26.3)\n",
      "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.10,>=3.9.5->langchain-pinecone) (1.9.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (5.4.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (0.1.141)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langchain-pinecone) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2020.6.20)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (4.66.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2.0.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-pinecone) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-pinecone) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-pinecone) (2.23.4)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain-pinecone) (3.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-pinecone) (3.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.2.6)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.54.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain-core) (5.4.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.1.141)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (2.0.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==2.1.1+cu121 in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision==0.16.1+cu121 in /usr/local/lib/python3.11/dist-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.1+cu121) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.1+cu121) (1.26.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.1+cu121) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.1+cu121) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.1+cu121) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.1+cu121) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.16.1+cu121) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.1+cu121) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision==0.16.1+cu121) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.1+cu121) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting huggingface_hub==0.11.1\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.11.1) (3.13.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.11.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.11.1) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub==0.11.1) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.11.1) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.11.1) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.11.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub==0.11.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.11.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub==0.11.1) (2020.6.20)\n",
      "Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.26.2\n",
      "    Uninstalling huggingface-hub-0.26.2:\n",
      "      Successfully uninstalled huggingface-hub-0.26.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 5.5.0 requires huggingface-hub>=0.25.1, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "gradio-client 1.4.2 requires huggingface-hub>=0.19.3, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "tokenizers 0.20.3 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "transformers 4.46.2 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "sentence-transformers 3.2.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "diffusers 0.21.4 requires huggingface-hub>=0.13.2, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "datasets 2.14.5 requires huggingface-hub<1.0.0,>=0.14.0, but you have huggingface-hub 0.11.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.11.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client) (2020.6.20)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.0.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.9.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.12.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.1.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.11.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytube openai-whisper langchain openai yt-dlp pinecone-client python-dotenv\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-pinecone\n",
    "!pip install -U langchain-openai langchain-core\n",
    "!pip install torch==2.1.1+cu121 torchvision==0.16.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install huggingface_hub==0.11.1\n",
    "!pip install -U pinecone-client\n",
    "!pip install pydantic\n",
    "!pip install python-dotenv\n",
    "!pip install -U sentence-transformers\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install langsmith\n",
    "!pip install --upgrade gradio\n",
    "!pip install flake8 black isort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Type, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent, initialize_agent\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key Loaded: True\n",
      "Pinecone API Key Loaded: True\n",
      "LangChain API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION ENVIRONMENT VARIABLES\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Define environment variables\n",
    "env_content = \"\"\"\n",
    "OPENAI_API_KEY=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\n",
    "PINECONE_API_KEY=53e1e922-7da6-4a86-90e7-4d61371838a4\n",
    "PINECONE_INDEX_NAME=lecture-embeddings\n",
    "PINECONE_INDEX_URL=https://lecture-embeddings-7moxkfr.svc.aped-4627-b74a.pinecone.io\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_API_KEY=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\n",
    "LANGCHAIN_PROJECT=pr-rash-improvement-74\n",
    "\"\"\"\n",
    "\n",
    "# Write environment variables\n",
    "env_file_path = \"/notebooks/.env\"\n",
    "with open(env_file_path, 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# Load and verify environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME')\n",
    "PINECONE_INDEX_URL = os.getenv('PINECONE_INDEX_URL')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Verify API keys are loaded\n",
    "print(\"OpenAI API Key Loaded:\", bool(OPENAI_API_KEY))\n",
    "print(\"Pinecone API Key Loaded:\", bool(PINECONE_API_KEY))\n",
    "print(\"LangChain API Key Loaded:\", bool(LANGCHAIN_API_KEY))\n",
    "\n",
    "# Initialize base clients\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUTUBE DOWNDLOAD AND TRANSCRIPTION SETUP\n",
    "# Set up storage paths\n",
    "NOTEBOOKS_PATH = \"/notebooks\"\n",
    "STORAGE_PATH = os.path.join(NOTEBOOKS_PATH, \"youtube_transcripts\")\n",
    "DOWNLOADS_DIR = os.path.join(STORAGE_PATH, \"downloads\")\n",
    "TRANSCRIPTS_DIR = os.path.join(STORAGE_PATH, \"transcripts\")\n",
    "MODEL_CACHE_DIR = os.path.join(STORAGE_PATH, \"model_cache\")\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [DOWNLOADS_DIR, TRANSCRIPTS_DIR, MODEL_CACHE_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Created directory: {directory}\")\n",
    "\n",
    "def read_urls_from_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Read URLs from a text file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        urls = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    return urls\n",
    "\n",
    "def download_audio(url: str, lecture_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Download audio from YouTube URL with sequential lecture numbering.\n",
    "    \"\"\"\n",
    "    output_template = os.path.join(DOWNLOADS_DIR, f'Lecture{lecture_number}')\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192'\n",
    "        }],\n",
    "        'outtmpl': output_template,\n",
    "        'quiet': False,\n",
    "        'no_warnings': False\n",
    "    }\n",
    "    \n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            ydl.download([url])\n",
    "            audio_path = f\"{output_template}.mp3\"\n",
    "            return audio_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def transcribe_audio(audio_path: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Transcribe audio and save the transcript to a file.\n",
    "    Returns tuple of (transcript text, transcript file path)\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting transcription process for {audio_path}\")\n",
    "    print(\"Loading Whisper model...\")\n",
    "    os.environ['WHISPER_CACHE_DIR'] = MODEL_CACHE_DIR\n",
    "    \n",
    "    try:\n",
    "        model = whisper.load_model(\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Model loaded successfully. Beginning transcription...\")\n",
    "        \n",
    "        result = model.transcribe(audio_path)\n",
    "        print(\"Transcription completed successfully\")\n",
    "        \n",
    "        base_name = os.path.basename(audio_path).replace('.mp3', '')\n",
    "        transcript_path = os.path.join(TRANSCRIPTS_DIR, f\"{base_name}_transcript.txt\")\n",
    "        \n",
    "        print(f\"Saving transcript to {transcript_path}\")\n",
    "        with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result['text'])\n",
    "        print(\"Transcript saved successfully\")\n",
    "        \n",
    "        return result['text'], transcript_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class BatchDownloadAndTranscribeTool(BaseTool):\n",
    "    name: str = \"Batch Download and Transcribe\"\n",
    "    description: str = \"Downloads audio from multiple YouTube URLs and transcribes them to text. Input can be either a single URL or path to a text file containing URLs.\"\n",
    "    \n",
    "    def _run(self, input_path_or_url: str) -> str:\n",
    "        \"\"\"Run the tool.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting batch process for input: {input_path_or_url}\")\n",
    "            \n",
    "            if input_path_or_url.endswith('.txt'):\n",
    "                urls = read_urls_from_file(input_path_or_url)\n",
    "                print(f\"Found {len(urls)} URLs in file\")\n",
    "            else:\n",
    "                urls = [input_path_or_url]\n",
    "            \n",
    "            results = []\n",
    "            for i, url in enumerate(urls, 1):\n",
    "                print(f\"\\nProcessing URL {i}/{len(urls)}: {url}\")\n",
    "                try:\n",
    "                    print(f\"Step 1: Downloading audio for Lecture {i}...\")\n",
    "                    audio_path = download_audio(url, i)\n",
    "                    print(f\"✓ Audio downloaded to: {audio_path}\")\n",
    "                    \n",
    "                    print(f\"Step 2: Transcribing Lecture {i}...\")\n",
    "                    transcript, transcript_path = transcribe_audio(audio_path)\n",
    "                    print(f\"✓ Transcript saved to: {transcript_path}\")\n",
    "                    \n",
    "                    audio_size = os.path.getsize(audio_path) / (1024 * 1024)\n",
    "                    transcript_size = os.path.getsize(transcript_path) / 1024\n",
    "                    \n",
    "                    results.append({\n",
    "                        'lecture_number': i,\n",
    "                        'url': url,\n",
    "                        'audio_path': audio_path,\n",
    "                        'audio_size': audio_size,\n",
    "                        'transcript_path': transcript_path,\n",
    "                        'transcript_size': transcript_size,\n",
    "                        'success': True\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Lecture {i}: {str(e)}\")\n",
    "                    results.append({\n",
    "                        'lecture_number': i,\n",
    "                        'url': url,\n",
    "                        'error': str(e),\n",
    "                        'success': False\n",
    "                    })\n",
    "            \n",
    "            report = \"\\nBatch Processing Summary:\\n\"\n",
    "            report += \"=\" * 50 + \"\\n\"\n",
    "            for result in results:\n",
    "                report += f\"\\nLecture {result['lecture_number']}:\\n\"\n",
    "                report += f\"URL: {result['url']}\\n\"\n",
    "                if result['success']:\n",
    "                    report += f\"Audio ({result['audio_size']:.2f}MB): {result['audio_path']}\\n\"\n",
    "                    report += f\"Transcript ({result['transcript_size']:.2f}KB): {result['transcript_path']}\\n\"\n",
    "                else:\n",
    "                    report += f\"Failed: {result['error']}\\n\"\n",
    "            \n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"An error occurred during batch processing: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    def _arun(self, url: str) -> str:\n",
    "        \"\"\"Run the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "# Initialize the tool and agent\n",
    "batch_download_tool = BatchDownloadAndTranscribeTool()\n",
    "\n",
    "# Helper function to format tools\n",
    "def format_tools(tools):\n",
    "    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "\n",
    "# Initialize memory for conversational context\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Define the prompt template with tool instructions\n",
    "template = \"\"\"You are a helpful assistant that processes YouTube videos for transcription.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    partial_variables={\"tools\": lambda x: format_tools(tools), \"tool_names\": lambda x: \", \".join(t.name for t in tools)}\n",
    ")\n",
    "\n",
    "# Initialize the agent with the tool\n",
    "tools = [batch_download_tool]\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Reset directories if needed\n",
    "for dir_path in [DOWNLOADS_DIR, TRANSCRIPTS_DIR]:\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"YouTube download and transcription setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSCRIPT PROCESSING\n",
    "\n",
    "# Define lecture information and metadata\n",
    "LECTURE_INFO = {\n",
    "    1: {\n",
    "        'title': 'Natural Language Processing with Deep Learning',\n",
    "        'main_topics': ['NLP basics', 'Word Vectors'],\n",
    "        'key_concepts': ['Natural Language Processing', 'Word Vectors', 'Singular Value Decomposition', \n",
    "                        'Skip-gram', 'Continuous Bag of Words', 'Negative Sampling', \n",
    "                        'Hierarchical Softmax', 'Word2Vec'],\n",
    "        'builds_on': []  # First lecture, no prerequisites\n",
    "    },\n",
    "    2: {\n",
    "        'title': 'Word Vector Representations: word2vec',\n",
    "        'main_topics': ['Word Vector Implementation', 'Word2Vec Details'],\n",
    "        'key_concepts': ['Word Vectors', 'Skip-gram', 'Continuous Bag of Words', \n",
    "                        'Negative Sampling', 'Hierarchical Softmax', 'Word2Vec'],\n",
    "        'builds_on': ['Word Vectors', 'Natural Language Processing']\n",
    "    },\n",
    "    3: {\n",
    "        'title': 'GloVe: Global Vectors for Word Representation',\n",
    "        'main_topics': ['GloVe', 'Word Vector Evaluation'],\n",
    "        'key_concepts': ['GloVe', 'Intrinsic evaluation', 'Extrinsic evaluation', \n",
    "                        'Word analogies', 'Context windows', 'Window classification'],\n",
    "        'builds_on': ['Word Vectors', 'Word2Vec']\n",
    "    },\n",
    "    4: {\n",
    "        'title': 'Word Window Classification and Neural Networks',\n",
    "        'main_topics': ['Neural Networks', 'Classification'],\n",
    "        'key_concepts': ['Neural networks', 'Forward computation', 'Backward propagation',\n",
    "                        'Neuron Units', 'Max-margin Loss', 'Gradient checks', \n",
    "                        'Xavier initialization', 'Learning rates', 'Adagrad'],\n",
    "        'builds_on': ['Window classification']\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_transcripts(transcripts_dir: str) -> Dict[int, str]:\n",
    "    \"\"\"Load all transcripts from the directory.\"\"\"\n",
    "    transcripts = {}\n",
    "    transcript_files = sorted(Path(transcripts_dir).glob(\"*_transcript.txt\"))\n",
    "    \n",
    "    for file_path in transcript_files:\n",
    "        if not str(file_path).endswith('.ipynb_checkpoints'):\n",
    "            lecture_num = int(file_path.name.split('_')[0].replace('Lecture', ''))\n",
    "            with open(file_path, 'r') as f:\n",
    "                transcripts[lecture_num] = f.read()\n",
    "    \n",
    "    print(f\"Loaded {len(transcripts)} transcripts\")\n",
    "    return transcripts\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = 1500, overlap: int = 200) -> List[Dict]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Determine end position of current chunk\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # If we're not at the end of the text, find the next period for clean breaks\n",
    "        if end < len(text):\n",
    "            next_period = text[end:min(end + 100, len(text))].find('.')\n",
    "            if next_period != -1:\n",
    "                end = end + next_period + 1\n",
    "        \n",
    "        chunk = text[start:end].strip()\n",
    "        chunks.append({\n",
    "            'content': chunk,\n",
    "            'char_start': start,\n",
    "            'char_end': end\n",
    "        })\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Main processing function\n",
    "def process_all_transcripts():\n",
    "    \"\"\"Process all transcripts and create structured chunks with metadata.\"\"\"\n",
    "    print(\"\\nStarting transcript processing...\")\n",
    "    \n",
    "    # Load all transcripts\n",
    "    transcripts = load_transcripts(TRANSCRIPTS_DIR)\n",
    "    \n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Process each lecture\n",
    "    for lecture_num, text in transcripts.items():\n",
    "        print(f\"\\nProcessing Lecture {lecture_num}...\")\n",
    "        \n",
    "        # Get lecture metadata\n",
    "        lecture_info = LECTURE_INFO.get(lecture_num, {\n",
    "            'title': f'Lecture {lecture_num}',\n",
    "            'key_concepts': [],\n",
    "            'main_topics': []\n",
    "        })\n",
    "        \n",
    "        # Create chunks for this lecture\n",
    "        chunks = create_chunks(text)\n",
    "        \n",
    "        # Add metadata to each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                'lecture_number': lecture_num,\n",
    "                'lecture_title': lecture_info['title'],\n",
    "                'key_concepts': lecture_info['key_concepts'],\n",
    "                'main_topics': lecture_info['main_topics'],\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks),\n",
    "                'content': chunk['content'],\n",
    "                'char_start': chunk['char_start'],\n",
    "                'char_end': chunk['char_end']\n",
    "            }\n",
    "            processed_chunks.append(chunk_data)\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks for Lecture {lecture_num}\")\n",
    "    \n",
    "    # Save processed chunks\n",
    "    output_dir = os.path.join(STORAGE_PATH, \"processed\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'lecture_chunks.json')\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(processed_chunks, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total chunks created: {len(processed_chunks)}\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "# Execute the processing\n",
    "if __name__ == \"__main__\":\n",
    "    processed_chunks = process_all_transcripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING GENERATION\n",
    "\n",
    "# Initialize OpenAI client for embeddings\n",
    "print(\"Initializing embedding generation...\")\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"Generate embedding for text using OpenAI's API\"\"\"\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_chunks_with_embeddings():\n",
    "    \"\"\"Process all chunks and add embeddings\"\"\"\n",
    "    # Load chunks\n",
    "    input_file = os.path.join(STORAGE_PATH, \"processed\", \"lecture_chunks.json\")\n",
    "    print(f\"Loading chunks from {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        print(f\"Loaded {len(chunks)} chunks\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Processed chunks file not found. Please run transcript processing first.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON file\")\n",
    "        return\n",
    "\n",
    "    # Process chunks and add embeddings\n",
    "    chunks_with_embeddings = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(\"\\nGenerating embeddings for all chunks...\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"Processing chunk {i}/{total_chunks}\", end='\\r')\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = get_embedding(chunk['content'])\n",
    "        \n",
    "        if embedding:\n",
    "            # Add embedding to chunk data\n",
    "            chunk_with_embedding = chunk.copy()\n",
    "            chunk_with_embedding['embedding'] = embedding\n",
    "            chunks_with_embeddings.append(chunk_with_embedding)\n",
    "        else:\n",
    "            print(f\"\\nWarning: Failed to generate embedding for chunk {i}\")\n",
    "\n",
    "    # Save embeddings\n",
    "    output_file = os.path.join(STORAGE_PATH, \"processed\", \"lecture_chunks_with_embeddings.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(chunks_with_embeddings, f)\n",
    "\n",
    "    print(f\"\\nEmbedding generation complete!\")\n",
    "    print(f\"Processed {len(chunks_with_embeddings)} chunks with embeddings\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    \n",
    "    # Print sample embedding dimension\n",
    "    if chunks_with_embeddings:\n",
    "        embedding_dim = len(chunks_with_embeddings[0]['embedding'])\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    return chunks_with_embeddings\n",
    "\n",
    "def verify_embeddings(chunks_with_embeddings):\n",
    "    \"\"\"Verify the quality and consistency of generated embeddings\"\"\"\n",
    "    if not chunks_with_embeddings:\n",
    "        print(\"No embeddings to verify\")\n",
    "        return False\n",
    "\n",
    "    expected_dim = 1536  # Expected dimension for OpenAI ada-002 embeddings\n",
    "    all_valid = True\n",
    "\n",
    "    print(\"\\nVerifying embeddings...\")\n",
    "    for i, chunk in enumerate(chunks_with_embeddings):\n",
    "        # Check if embedding exists\n",
    "        if 'embedding' not in chunk:\n",
    "            print(f\"Chunk {i} missing embedding\")\n",
    "            all_valid = False\n",
    "            continue\n",
    "\n",
    "        # Check embedding dimension\n",
    "        embedding_dim = len(chunk['embedding'])\n",
    "        if embedding_dim != expected_dim:\n",
    "            print(f\"Chunk {i} has incorrect dimension: {embedding_dim} (expected {expected_dim})\")\n",
    "            all_valid = False\n",
    "\n",
    "        # Check for null values\n",
    "        if None in chunk['embedding']:\n",
    "            print(f\"Chunk {i} contains null values in embedding\")\n",
    "            all_valid = False\n",
    "\n",
    "    if all_valid:\n",
    "        print(\"✓ All embeddings verified successfully!\")\n",
    "    else:\n",
    "        print(\"× Some embeddings failed verification\")\n",
    "\n",
    "    return all_valid\n",
    "\n",
    "# Execute embedding generation and verification\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting embedding generation process...\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    chunks_with_embeddings = process_chunks_with_embeddings()\n",
    "    \n",
    "    # Verify embeddings\n",
    "    if chunks_with_embeddings:\n",
    "        verify_embeddings(chunks_with_embeddings)\n",
    "    \n",
    "    print(\"\\nEmbedding process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Pinecone...\n",
      "Connected to existing Pinecone index: lecture-embeddings\n",
      "\n",
      "Pinecone Index Statistics:\n",
      "Total vectors: 311\n",
      "Dimension: 1536\n",
      "\n",
      "Test query successful!\n",
      "Sample document metadata:\n",
      "\n",
      "Metadata fields found:\n",
      "chunk_index: 9.0\n",
      "content: <content length: 1500 chars>\n",
      "key_concepts: ['Neural Networks', 'Classification', 'Backpropagation']\n",
      "lecture_number: 4.0\n",
      "lecture_title: Word Window Classification and Neural Networks\n",
      "page_content: tors. And we had trained these ward vectors on a very, very large corpus. And it learned all these three words appear often in similar context, so they're close by in the vector space. And now we're going to train, but our smaller sentiment data set only includes in a training set, the X, Y, Y, S, TV and telly and not television. So now what happens as we train these ward vectors? Well, they will start to move around. We'll project sentiment into them. And so you now might see telly and TV, so it's a pretty status set. So like to move somewhere else into the vector space, but television actually stays where it was in the beginning. And now when we want to test it, we would actually now misclassify this word because it's never been moved. And so what does that mean? The take home message here will be that if you have only a very small training data set that will allow you, especially with these deep models to overfit very quickly, you do not want to train your ward vectors. You want to keep them fixed, you pre-trained them with nice glove or were to veck models on a very large corpus or you just downloaded them from the glove website and you want to keep them fixed. Otherwise, you will not generalize as well. However, if you have a very large data set, it may be better to train them in the way we're going to describe in the next couple of slides. So an example for where you do that is for instance, machine translation where you might have many hundreds of megabytes or gigabyte\n",
      "\n",
      "LangChain vector store initialized successfully\n",
      "Vector store retrieval test successful\n",
      "Retrieved document length: 1500\n",
      "\n",
      "Pinecone setup completed successfully!\n",
      "\n",
      "Setup complete! The vector store is ready for querying.\n",
      "\n",
      "Performing final verification...\n",
      "✅ Vector store is working correctly\n",
      "Sample document metadata keys: ['chunk_index', 'key_concepts', 'lecture_number', 'lecture_title', 'page_content']\n"
     ]
    }
   ],
   "source": [
    "# PINECONE SET UP AND VECTOR STORAGE\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from tqdm import tqdm\n",
    "\n",
    "def initialize_pinecone(index_name):\n",
    "    \"\"\"Initialize Pinecone client and connect to index\"\"\"\n",
    "    print(\"\\nInitializing Pinecone...\")\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        \n",
    "        # Check if index exists, if not create it\n",
    "        try:\n",
    "            index = pc.Index(index_name)\n",
    "            print(f\"Connected to existing Pinecone index: {index_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Index not found, creating new index: {index_name}\")\n",
    "            # Create index with serverless spec\n",
    "            spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=1536,  # OpenAI ada-002 embedding dimension\n",
    "                metric=\"cosine\",\n",
    "                spec=spec\n",
    "            )\n",
    "            index = pc.Index(index_name)\n",
    "            print(f\"Created new Pinecone index: {index_name}\")\n",
    "        \n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_vectors_for_upsert(chunks_with_embeddings):\n",
    "    \"\"\"Prepare vectors in the format required by Pinecone\"\"\"\n",
    "    vectors = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_with_embeddings):\n",
    "        # Create unique ID for each vector\n",
    "        vector_id = f\"chunk_{chunk['lecture_number']}_{chunk['chunk_index']}\"\n",
    "        \n",
    "        # Prepare metadata - Store main content in 'content' field\n",
    "        metadata = {\n",
    "            'lecture_number': chunk['lecture_number'],\n",
    "            'lecture_title': chunk['lecture_title'],\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'key_concepts': chunk['key_concepts'],\n",
    "            'content': chunk['content']  # Main content stored here\n",
    "        }\n",
    "        \n",
    "        # Create vector object\n",
    "        vector = {\n",
    "            'id': vector_id,\n",
    "            'values': chunk['embedding'],\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def upsert_to_pinecone(index, vectors, batch_size=100):\n",
    "    \"\"\"Upsert vectors to Pinecone in batches\"\"\"\n",
    "    print(\"\\nUpserting vectors to Pinecone...\")\n",
    "    total_vectors = len(vectors)\n",
    "    \n",
    "    for i in range(0, total_vectors, batch_size):\n",
    "        batch = vectors[i:min(i + batch_size, total_vectors)]\n",
    "        try:\n",
    "            index.upsert(vectors=batch)\n",
    "            print(f\"Upserted batch {i//batch_size + 1}/{(total_vectors + batch_size - 1)//batch_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error upserting batch starting at index {i}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def verify_pinecone_index(index):\n",
    "    \"\"\"Verify the index statistics and content\"\"\"\n",
    "    try:\n",
    "        # Get index stats\n",
    "        stats = index.describe_index_stats()\n",
    "        \n",
    "        print(\"\\nPinecone Index Statistics:\")\n",
    "        print(f\"Total vectors: {stats.total_vector_count}\")\n",
    "        print(f\"Dimension: {stats.dimension}\")\n",
    "        \n",
    "        # Perform a test query with embedding\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        test_query = embedding_model.embed_query(\"test\")\n",
    "        \n",
    "        results = index.query(\n",
    "            vector=test_query,\n",
    "            top_k=1,\n",
    "            include_values=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        if results.matches:\n",
    "            print(\"\\nTest query successful!\")\n",
    "            print(\"Sample document metadata:\")\n",
    "            metadata = results.matches[0].metadata\n",
    "            print(\"\\nMetadata fields found:\")\n",
    "            for key in metadata.keys():\n",
    "                if key == 'content':\n",
    "                    print(f\"{key}: <content length: {len(str(metadata[key]))} chars>\")\n",
    "                else:\n",
    "                    print(f\"{key}: {metadata[key]}\")\n",
    "        else:\n",
    "            print(\"Test query returned no results\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying index: {e}\")\n",
    "        return False\n",
    "\n",
    "def initialize_vector_store(index):\n",
    "    \"\"\"Initialize LangChain's vector store wrapper for Pinecone\"\"\"\n",
    "    try:\n",
    "        vector_store = PineconeVectorStore(\n",
    "            index=index,\n",
    "            embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "            text_key=\"content\"  # Using 'content' as the text key\n",
    "        )\n",
    "        print(\"\\nLangChain vector store initialized successfully\")\n",
    "        \n",
    "        # Verify vector store works\n",
    "        test_results = vector_store.similarity_search(\"test\", k=1)\n",
    "        if test_results:\n",
    "            print(\"Vector store retrieval test successful\")\n",
    "            print(f\"Retrieved document length: {len(test_results[0].page_content)}\")\n",
    "        \n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_pinecone_setup(mode='connect'):\n",
    "    \"\"\"Main function to set up Pinecone and upload vectors\"\"\"\n",
    "    # Initialize Pinecone\n",
    "    index = initialize_pinecone(PINECONE_INDEX_NAME)\n",
    "    if not index:\n",
    "        return False\n",
    "        \n",
    "    if mode == 'new':\n",
    "        # Load chunks with embeddings\n",
    "        input_file = os.path.join(STORAGE_PATH, \"processed\", \"lecture_chunks_with_embeddings.json\")\n",
    "        try:\n",
    "            with open(input_file, 'r') as f:\n",
    "                chunks_with_embeddings = json.load(f)\n",
    "            print(f\"\\nLoaded {len(chunks_with_embeddings)} chunks with embeddings\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings file: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Prepare and upsert vectors\n",
    "        vectors = prepare_vectors_for_upsert(chunks_with_embeddings)\n",
    "        print(f\"Prepared {len(vectors)} vectors for upload\")\n",
    "        \n",
    "        if not upsert_to_pinecone(index, vectors):\n",
    "            return False\n",
    "    \n",
    "    # Verify the index\n",
    "    if not verify_pinecone_index(index):\n",
    "        return False\n",
    "    \n",
    "    # Initialize vector store\n",
    "    vector_store = initialize_vector_store(index)\n",
    "    if not vector_store:\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nPinecone setup completed successfully!\")\n",
    "    return vector_store\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose setup mode: 'new' for fresh setup, 'connect' for existing index\n",
    "    setup_mode = 'connect'  # or 'new'\n",
    "    \n",
    "    vector_store = main_pinecone_setup(mode=setup_mode)\n",
    "    \n",
    "    if vector_store:\n",
    "        print(\"\\nSetup complete! The vector store is ready for querying.\")\n",
    "        \n",
    "        # Verification step\n",
    "        print(\"\\nPerforming final verification...\")\n",
    "        try:\n",
    "            test_docs = vector_store.similarity_search(\"test query\", k=1)\n",
    "            if test_docs:\n",
    "                print(\"✅ Vector store is working correctly\")\n",
    "                print(f\"Sample document metadata keys: {list(test_docs[0].metadata.keys())}\")\n",
    "            else:\n",
    "                print(\"⚠️ Vector store returned no results\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in final verification: {e}\")\n",
    "    else:\n",
    "        print(\"\\nSetup failed. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Connecting to Pinecone...\n",
      "2. Accessing index: lecture-embeddings\n",
      "3. Creating test query...\n",
      "4. Performing test query...\n",
      "\n",
      "✅ Connection successful!\n",
      "\n",
      "Sample document metadata:\n",
      "chunk_index: 1.0\n",
      "content: <text length: 1500 chars>\n",
      "Preview:  start with sort of vectors and derivatives and chain rules and all of that stuff. So you should get ...\n",
      "key_concepts: ['NLP', 'Word Vectors', 'Word2Vec']\n",
      "lecture_number: 1.0\n",
      "lecture_title: Natural Language Processing with Deep Learning\n",
      "page_content: <text length: 1500 chars>\n",
      "Preview:  start with sort of vectors and derivatives and chain rules and all of that stuff. So you should get ...\n"
     ]
    }
   ],
   "source": [
    "# PINECONE CONNECTION TESTING UTILITY\n",
    "\n",
    "def verify_pinecone_connection():\n",
    "    \"\"\"\n",
    "    Verify Pinecone connection and index functionality.\n",
    "    Tests connection, queries, and metadata retrieval.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        print(\"\\n1. Connecting to Pinecone...\")\n",
    "        pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "        \n",
    "        # Get index\n",
    "        index_name = os.environ[\"PINECONE_INDEX_NAME\"]\n",
    "        print(f\"2. Accessing index: {index_name}\")\n",
    "        index = pc.Index(index_name)\n",
    "        \n",
    "        # Create test vector\n",
    "        print(\"3. Creating test query...\")\n",
    "        test_vector = [0.0] * 1536  # OpenAI embeddings dimension\n",
    "        \n",
    "        # Perform query\n",
    "        print(\"4. Performing test query...\")\n",
    "        response = index.query(\n",
    "            vector=test_vector,\n",
    "            top_k=1,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        if response and response.matches:\n",
    "            print(\"\\n✅ Connection successful!\")\n",
    "            print(\"\\nSample document metadata:\")\n",
    "            match = response.matches[0]\n",
    "            for key, value in match.metadata.items():\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"{key}: <text length: {len(value)} chars>\")\n",
    "                    print(\"Preview:\", value[:100], \"...\")\n",
    "                else:\n",
    "                    print(f\"{key}:\", value)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"\\n⚠️ Connection successful but no documents found\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_pinecone_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://49388f5b7d6d092856.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://49388f5b7d6d092856.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question: what does NLP mean?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 relevant lectures\n",
      "Found 3 key concepts\n",
      "\n",
      "Processing question: What is cross entropy?\n",
      "Found 3 relevant lectures\n",
      "Found 9 key concepts\n",
      "\n",
      "Processing question: How is it used in neural networks?\n",
      "Found 3 relevant lectures\n",
      "Found 9 key concepts\n",
      "\n",
      "Processing question: What other loss functions are mentioned in the neural networks lecture?\n",
      "\n",
      "Processing question: What is BERT?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question: What methods for word representation ARE covered in the lectures?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `content` key. Skipping.\n",
      "Found document with no `content` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant lectures\n",
      "Found 5 key concepts\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import List\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize components \n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index(os.environ[\"PINECONE_INDEX_NAME\"])\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"content\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=3,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Modified prompt template to enforce strict retrieval\n",
    "custom_template = \"\"\"\n",
    "You are a helpful teaching assistant for a course on Computational Linguistics.\n",
    "You must ONLY answer using information from the provided context. \n",
    "If the context doesn't contain enough information to answer the question properly, respond with \"I can't find specific information about this in the course materials.\"\n",
    "Do not use any external knowledge or make assumptions beyond what's in the context.\n",
    "\n",
    "Current conversation:\n",
    "{chat_history}\n",
    "\n",
    "Context from course materials:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Check if the context contains relevant information to answer the question\n",
    "2. If sufficient information exists:\n",
    "   - Connect your answer to specific lectures\n",
    "   - Use only information from the provided context\n",
    "   - Include key concepts mentioned in the context\n",
    "3. If insufficient information exists:\n",
    "   - Respond with \"I can't find specific information about this in the course materials.\"\n",
    "4. Never make up or infer information not present in the context\n",
    "\n",
    "Student Question: {question}\n",
    "\n",
    "Teaching Assistant Answer:\"\"\"\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    template=custom_template, \n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 8}),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': CUSTOM_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def chat(message: str) -> str:\n",
    "    \"\"\"Process a single message and return the response\"\"\"\n",
    "    print(f\"\\nProcessing question: {message}\")\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    try:\n",
    "        # Get response\n",
    "        response = qa_chain({\"question\": message})\n",
    "        \n",
    "        # Check if we got any source documents\n",
    "        if not response.get('source_documents'):\n",
    "            return \"I can't find specific information about this in the course materials.\"\n",
    "        \n",
    "        # Process the response\n",
    "        answer = response[\"answer\"].strip()\n",
    "        \n",
    "        # If the answer indicates no information was found, return early\n",
    "        if \"can't find specific information\" in answer.lower():\n",
    "            return answer\n",
    "        \n",
    "        # Add sources and concepts if we have a valid answer\n",
    "        sources = set()\n",
    "        concepts = set()\n",
    "        \n",
    "        for doc in response['source_documents']:\n",
    "            if hasattr(doc, 'metadata'):\n",
    "                lecture_num = doc.metadata.get('lecture_number')\n",
    "                lecture_title = doc.metadata.get('lecture_title')\n",
    "                if lecture_num and lecture_title:\n",
    "                    sources.add(f\"Lecture {lecture_num}: {lecture_title}\")\n",
    "                \n",
    "                if 'key_concepts' in doc.metadata:\n",
    "                    concepts.update(doc.metadata['key_concepts'])\n",
    "        \n",
    "        # Add metadata to response\n",
    "        formatted_response = answer\n",
    "        if sources:\n",
    "            formatted_response += \"\\n\\n📚 Sources:\\n\" + \"\\n\".join(f\"• {s}\" for s in sorted(sources))\n",
    "        if concepts:\n",
    "            formatted_response += \"\\n\\n🔑 Key Concepts:\\n• \" + \", \".join(sorted(concepts))\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"Found {len(sources)} relevant lectures\")\n",
    "        print(f\"Found {len(concepts)} key concepts\")\n",
    "        \n",
    "        return formatted_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return f\"❌ Error: {str(e)}\\nPlease try asking your question again.\"\n",
    "\n",
    "# Create the Gradio interface - Fixed indentation\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Friendly header\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🎓 Computational Linguistics Teaching Assistant\n",
    "    \n",
    "    Welcome! I'm your CS teaching assistant, trained on Stanford's lectures. \n",
    "    I can help you understand:\n",
    "    - 📚 Word Vectors and Embeddings\n",
    "    - 🧠 Neural Networks in NLP\n",
    "    - 🤖 GloVe and Word Representations\n",
    "    \n",
    "    Ask me anything about these topics!\n",
    "    \"\"\")\n",
    "\n",
    "    # Main interface\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            question = gr.Textbox(\n",
    "                placeholder=\"🌟 No question is too simple or too complex - I'm here to help!\",\n",
    "                label=\"Your Question\",\n",
    "                lines=3\n",
    "            )\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            answer = gr.Textbox(\n",
    "                label=\"Assistant Response\",\n",
    "                lines=15,\n",
    "                interactive=False\n",
    "            )\n",
    "\n",
    "    # Handle events\n",
    "    def clear_fields():\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    submit.click(fn=chat, inputs=question, outputs=answer)\n",
    "    question.submit(fn=chat, inputs=question, outputs=answer)\n",
    "    clear.click(fn=clear_fields, inputs=[], outputs=[question, answer])\n",
    "\n",
    "    # Example Questions\n",
    "    gr.Markdown(\"### 💡 Not sure where to start?\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"🤔 What is cross entropy?\",\n",
    "            \"✨ How do neural networks work in NLP?\",\n",
    "            \"🔍 What is GloVe and what problem does it solve?\",\n",
    "        ],\n",
    "        inputs=question,\n",
    "        label=\"Try these questions\"\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
